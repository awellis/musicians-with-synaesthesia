@misc{burknerBayesianItemResponse2020c,
  title = {Bayesian {{Item Response Modeling}} in {{R}} with Brms and {{Stan}}},
  author = {Bürkner, Paul-Christian},
  date = {2020-02-01},
  number = {arXiv:1905.09501},
  eprint = {1905.09501},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.09501},
  url = {http://arxiv.org/abs/1905.09501},
  urldate = {2023-01-19},
  abstract = {Item Response Theory (IRT) is widely applied in the human sciences to model persons' responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective prespecified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. We demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and post-processed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation}
}

@article{burknerOrdinalRegressionModels2019,
  title = {Ordinal {{Regression Models}} in {{Psychology}}: {{A Tutorial}}},
  shorttitle = {Ordinal {{Regression Models}} in {{Psychology}}},
  author = {Bürkner, Paul-Christian and Vuorre, Matti},
  date = {2019-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {1},
  pages = {77--101},
  issn = {2515-2459},
  doi = {10.1177/2515245918823199},
  url = {https://doi.org/10.1177/2515245918823199},
  urldate = {2019-06-11},
  abstract = {Ordinal variables, although extremely common in psychology, are almost exclusively analyzed with statistical models that falsely assume them to be metric. This practice can lead to distorted effect-size estimates, inflated error rates, and other problems. We argue for the application of ordinal models that make appropriate assumptions about the variables under study. In this Tutorial, we first explain the three major classes of ordinal models: the cumulative, sequential, and adjacent-category models. We then show how to fit ordinal models in a fully Bayesian framework with the R package brms, using data sets on opinions about stem-cell research and time courses of marriage. The appendices provide detailed mathematical derivations of the models and a discussion of censored ordinal models. Compared with metric models, ordinal models provide better theoretical interpretation and numerical inference from ordinal data, and we recommend their widespread adoption in psychology.},
  langid = {english}
}

@article{franzStandardErrorsConfidence2012a,
  title = {Standard Errors and Confidence Intervals in Within-Subjects Designs: {{Generalizing Loftus}} and {{Masson}} (1994) and Avoiding the Biases of Alternative Accounts},
  shorttitle = {Standard Errors and Confidence Intervals in Within-Subjects Designs},
  author = {Franz, Volker H. and Loftus, Geoffrey R.},
  date = {2012},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {19},
  number = {3},
  eprint = {22441956},
  eprinttype = {pmid},
  pages = {395--404},
  issn = {1069-9384},
  doi = {10.3758/s13423-012-0230-1},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3348489/},
  urldate = {2023-01-19},
  abstract = {Repeated measures designs are common in experimental psychology. Because of the correlational structure in these designs, the calculation and interpretation of confidence intervals is nontrivial. One solution was provided by Loftus and Masson (Psychonomic Bulletin \& Review 1:476–490, ). This solution, although widely adopted, has the limitation of implying same-size confidence intervals for all factor levels, and therefore does not allow for the assessment of variance homogeneity assumptions (i.e., the circularity assumption, which is crucial for the repeated measures ANOVA). This limitation and the method’s perceived complexity have sometimes led scientists to use a simplified variant, based on a per-subject normalization of the data (Bakeman \& McArthur, Behavior Research Methods, Instruments, \& Computers 28:584–589, ; Cousineau, Tutorials in Quantitative Methods for Psychology 1:42–45, ; Morey, Tutorials in Quantitative Methods for Psychology 4:61–64, ; Morrison \& Weaver, Behavior Research Methods, Instruments, \& Computers 27:52–56, ). We show that this normalization method leads to biased results and is uninformative with regard to circularity. Instead, we provide a simple, intuitive generalization of the Loftus and Masson method that allows for assessment of the circularity assumption.},
  pmcid = {PMC3348489}
}

@article{liddellAnalyzingOrdinalData2018a,
  title = {Analyzing Ordinal Data with Metric Models: {{What}} Could Possibly Go Wrong?},
  shorttitle = {Analyzing Ordinal Data with Metric Models},
  author = {Liddell, Torrin M. and Kruschke, John K.},
  date = {2018-11-01},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  volume = {79},
  pages = {328--348},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2018.08.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0022103117307746},
  urldate = {2022-12-16},
  abstract = {We surveyed all articles in the Journal of Personality and Social Psychology (JPSP), Psychological Science (PS), and the Journal of Experimental Psychology: General (JEP:G) that mentioned the term “Likert,” and found that 100\% of the articles that analyzed ordinal data did so using a metric model. We present novel evidence that analyzing ordinal data as if they were metric can systematically lead to errors. We demonstrate false alarms (i.e., detecting an effect where none exists, Type I errors) and failures to detect effects (i.e., loss of power, Type II errors). We demonstrate systematic inversions of effects, for which treating ordinal data as metric indicates the opposite ordering of means than the true ordering of means. We show the same problems — false alarms, misses, and inversions — for interactions in factorial designs and for trend analyses in regression. We demonstrate that averaging across multiple ordinal measurements does not solve or even ameliorate these problems. A central contribution is a graphical explanation of how and when the misrepresentations occur. Moreover, we point out that there is no sure-fire way to detect these problems by treating the ordinal values as metric, and instead we advocate use of ordered-probit models (or similar) because they will better describe the data. Finally, although frequentist approaches to some ordered-probit models are available, we use Bayesian methods because of their flexibility in specifying models and their richness and accuracy in providing parameter estimates. An R script is provided for running an analysis that compares ordered-probit and metric models.},
  langid = {english},
  keywords = {Bayesian analysis,Likert,Ordered-probit,Ordinal data}
}

@article{moreyConfidenceIntervalsNormalized2008a,
  title = {Confidence {{Intervals}} from {{Normalized Data}}: {{A}} Correction to {{Cousineau}} (2005)},
  shorttitle = {Confidence {{Intervals}} from {{Normalized Data}}},
  author = {Morey, Richard D.},
  date = {2008-09-01},
  journaltitle = {Tutorials in Quantitative Methods for Psychology},
  shortjournal = {TQMP},
  volume = {4},
  number = {2},
  pages = {61--64},
  issn = {1913-4126},
  doi = {10.20982/tqmp.04.2.p061},
  url = {http://www.tqmp.org/RegularArticles/vol04-2/p061},
  urldate = {2023-01-19},
  langid = {english}
}

@misc{paulewiczGeneralCausalCumulative2022,
  title = {The General Causal Cumulative Model of Ordinal Response},
  author = {Paulewicz, Borysław and Blaut, Agata},
  date = {2022-02-24T09:24:49},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/e7a3x},
  url = {https://psyarxiv.com/e7a3x/},
  urldate = {2022-11-16},
  abstract = {In this paper, we introduce the general causal cumulative model of ordinal response. The statistical part of this model is a new family of hierarchical or non-hierarchical generalized linear models that represent the distribution of the outcome as a thresholded latent distribution. Its defining feature is the new link functions, which are order-preserving in the sense that they allow for arbitrary effects in individual thresholds while preserving their order. We show how the model can be interpreted as a generalization of some Signal Detection Theory (SDT) models and some Item Response Theory models. We propose an approach to measurement of latent variables which seems to follow from the requirement that measurements should be interpreted in the context of a causal theory of the unobservable response process. In particular, we formulate a causal definition of measurement invariance that seems to match the examples of measurement bias found in the literature, and we show that the commonly accepted statistical definition is flawed; this leads to the recognition of the fundamental problem associated with item parameters and of the critical role of substantive theory and speculative reasoning. We illustrate how, by making use of the statistical properties of our model, this approach to measurement can be followed in practice, and we explain what kind of issues of a more technical nature it may help address and when. In particular, we identify the central causal assumption of SDT models, we show how this assumption can be tested, how introducing item parameters can lead to severely biased point and interval estimates of the target causal quantities, and how causal estimate bias can be accounted for by means of a causal sensitivity analysis.},
  langid = {american},
  keywords = {alternative causal explanations,Bayesian inference,causal inference,generalized linear models,hierarchical models,item response theory,measurement invariance,ordinal scale,Psychometrics,Quantitative Methods,signal detection theory,Social and Behavioral Sciences}
}
